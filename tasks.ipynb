{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd754c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d22148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.8/11.1 MB 29.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 28.9 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "   ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 7.3/13.0 MB 37.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.0/13.0 MB 37.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.1 pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef79f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (0.3.68)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.12.13-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (0.4.4)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-community) (2.3.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.6.3-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.4.1)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 24.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.12.13-cp311-cp311-win_amd64.whl (451 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.6.3-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
      "\n",
      "   ---------- -----------------------------  4/15 [marshmallow]\n",
      "   --------------------- ------------------  8/15 [yarl]\n",
      "   ----------------------------- ---------- 11/15 [pydantic-settings]\n",
      "   ---------------------------------- ----- 13/15 [aiohttp]\n",
      "   ---------------------------------- ----- 13/15 [aiohttp]\n",
      "   ---------------------------------- ----- 13/15 [aiohttp]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ------------------------------------- -- 14/15 [langchain-community]\n",
      "   ---------------------------------------- 15/15 [langchain-community]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.7.0 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 propcache-0.3.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af6d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b9504d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ford', 'Mustang', 1964)\n",
      "('Mercedes', 'G-Wagon', 1979)\n",
      "('Mercedes', 'Benz', 1926)\n",
      "('BMW', 'M1', 1978)\n",
      "('Toyota', 'Supra', 1978)\n",
      "('Volvo', 'p1800', 1968)\n",
      "('Toyota', 'Celica', 1975)\n",
      "('Rolls Royce', 'Phantom', 2003)\n",
      "('Rolls Royce', 'Ghost', 2009)\n",
      "('Rolls Royce', 'Culinan', 2019)\n",
      "('Honda', 'Civic', 2007)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib.parse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with your actual credentials\n",
    "user = \"postgres\"\n",
    "password = os.getenv(\"password\")\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "db_name = \"job_role\"\n",
    "\n",
    "# URL-encode the password\n",
    "encoded_password = urllib.parse.quote_plus(password)\n",
    "\n",
    "# SQLAlchemy connection string\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{encoded_password}@{host}:{port}/{db_name}\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT * FROM cars\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set your Groq API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"  # Groq uses OpenAI-compatible endpoint\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",   \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e67ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should look at the tables in the database to see what I can query.  Then I should query the schema of the most relevant tables.\n",
      "Action: sql_db_list_tables\n",
      "Action Input: \u001b[0m\u001b[38;5;200m\u001b[1;3mcars\u001b[0m\u001b[32;1m\u001b[1;3mI have the list of tables in the database, and I see that there is a table called \"cars\". This is likely to be the most relevant table for my query.\n",
      "\n",
      "Action: sql_db_schema\n",
      "Action Input: cars\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE cars (\n",
      "\tbrand VARCHAR(255), \n",
      "\tmodel VARCHAR(255), \n",
      "\tyear INTEGER\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from cars table:\n",
      "brand\tmodel\tyear\n",
      "Ford\tMustang\t1964\n",
      "Mercedes\tG-Wagon\t1979\n",
      "Mercedes\tBenz\t1926\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3mI have the schema of the \"cars\" table, and I see that it has columns for \"brand\", \"model\", and \"year\". I can use this information to construct a query to find Rolls Royce cars in the database.\n",
      "\n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT brand, model FROM cars WHERE brand = 'Rolls Royce' LIMIT 10;\u001b[0m\u001b[36;1m\u001b[1;3mSELECT \"brand\", \"model\" FROM \"cars\" WHERE \"brand\" = 'Rolls Royce' LIMIT 10;\u001b[0m\u001b[32;1m\u001b[1;3mThought: My query looks good, I'll execute it to get the results.\n",
      "\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT brand, model FROM cars WHERE brand = 'Rolls Royce' LIMIT 10;\u001b[0m\u001b[36;1m\u001b[1;3m[('Rolls Royce', 'Phantom'), ('Rolls Royce', 'Ghost'), ('Rolls Royce', 'Culinan')]\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Rolls Royce cars in the database are Phantom, Ghost, and Culinan.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Rolls Royce cars in the database are Phantom, Ghost, and Culinan.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain.prompts import SystemMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "# System Prompt\n",
    "custom_prompt = SystemMessage(content=\"\"\"\n",
    "You are a funny car expert helping users explore a car database. \n",
    "You should convert their questions into correct SQL queries and return relevant answers.\n",
    "If the query is ambiguous, ask for clarification.\n",
    "Be concise and helpful.\n",
    "\"\"\")\n",
    "\n",
    "#Create the toolkit and agent\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True,\n",
    "    system_message=custom_prompt   \n",
    ")\n",
    "\n",
    "#Ask the user for input\n",
    "query = input(\"Ask your car-related question: \")\n",
    "print(agent_executor.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set your Groq API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"  # Groq uses OpenAI-compatible endpoint\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3-70b-8192\",   \n",
    "    temperature=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46ca1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Resume Bullet:\n",
      " Here are the 2-word technical bullet points for each achievement:\n",
      "\n",
      "* **Chunking Strategy**\n",
      "* **LLaMA Integration**\n",
      "* **Custom Workflow**\n",
      "* **NLP Fine-tuning**\n",
      "* **Training Optimization**\n",
      "* **CNN Architectures**\n",
      "\n",
      "These bullet points are concise, technical, and highlight the specific skills and technologies used to achieve the described accomplishments.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are a professional resume writer.\\n\"\n",
    "    \"Given the job role and a description of an achievement, write 2 word technical bullet points.\\n\"\n",
    "    \"Role: {role}\\n\"\n",
    "    \"Achievement: {achievement}\\n\"\n",
    "    \"Resume Bullet:\"\n",
    ")\n",
    "\n",
    "llm = llm\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Setting up the chain\n",
    "bullet_chain = prompt | llm | parser\n",
    "\n",
    "# input\n",
    "input_data = {\n",
    "    \"role\": \"AI ML Intern\",\n",
    "    \"achievement\": [\n",
    "        \"Engineered document chunking strategy to optimize context window for large-document queries.\",\n",
    "        \"Integrated LLaMA 3.2 into a RAG pipeline to improve document-based answer generation.\",\n",
    "        \"Built a custom workflow for extracting business card data on a local server using DIFY.\",\n",
    "        \"Improved sentiment classification accuracy from 68% to 89% using NLP fine-tuning.\",\n",
    "        \"Reduced training time in CNNs for medical image classification by 40%.\",\n",
    "        \"Boosted diagnostic accuracy to 92% using custom CNN architectures in TensorFlow.\",\n",
    "        ]\n",
    "}\n",
    "\n",
    "# run the chain\n",
    "result = bullet_chain.invoke(input_data)\n",
    "\n",
    "# Output\n",
    "print(\"Generated Resume Bullet:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f76014f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills:\n",
      "\n",
      "Here are the extracted skills:\n",
      "\n",
      "**Technical Skills:**\n",
      "\n",
      "* Python\n",
      "* SQL\n",
      "* AWS\n",
      "* GCP\n",
      "* Docker\n",
      "* Git\n",
      "* TensorFlow\n",
      "* ML pipelines\n",
      "\n",
      "**Soft Skills:**\n",
      "\n",
      "* Problem-solving\n",
      "* Attention to detail\n",
      "* Ability to work independently\n",
      "* Collaboration\n",
      "* Communication\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are an AI assistant that extracts skills from job descriptions.\\n\"\n",
    "    \"Given the following job description, return two lists:\\n\"\n",
    "    \"1. Technical Skills\\n\"\n",
    "    \"2. Soft Skills\\n\\n\"\n",
    "    \"Job Description:\\n{jd}\\n\\n\"\n",
    "    \"Respond only with the categorized skills.\"\n",
    ")\n",
    "\n",
    "llm = llm\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Setting up the chain\n",
    "skill_extractor_chain  = prompt | llm | parser\n",
    "\n",
    "# input\n",
    "input_data = {\n",
    "    \"jd\" : \"We are looking for a Data Scientist to join our analytics team. Responsibilities include building predictive models, collaborating with cross-functional teams, communicating insights to stakeholders, and maintaining scalable ML pipelines. Must be proficient in Python, SQL, and cloud platforms like AWS or GCP. Experience with Docker, Git, and TensorFlow is a plus. Strong problem-solving skills, attention to detail, and the ability to work independently are essential.\"\n",
    "}\n",
    "\n",
    "# run the chain\n",
    "result = skill_extractor_chain.invoke(input_data)\n",
    "\n",
    "# Output\n",
    "print(\"Extracted Skills:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb7afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fapienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
