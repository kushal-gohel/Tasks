{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a740b4",
   "metadata": {},
   "source": [
    "## Document Loader using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f24856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69a099",
   "metadata": {},
   "source": [
    "### PDF LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import groq\n",
    "import sys\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY_temp_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15158368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"C:/Users/KUSHAL/OneDrive/Desktop/Notebook LM/thebook_machine_learning.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b7d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published by the press syndicate of the university of cambridge\n",
      "The Pitt Building, Trumpington Street, Cambridge, United Kingdom\n",
      "cambridge university press\n",
      "The Edinburgh Building, Cambridge CB2 2RU, UK\n",
      "40 West 20th Street, New York, NY 10011–4211, USA\n",
      "477 Williamstown Road, Port Melbourne, VIC 3207, Australia\n",
      "Ruiz de Alarc´ on 13, 28014 Madrid, Spain\n",
      "Dock House, The Waterfront, Cape Town 8001, South Africa\n",
      "http://www.cambridge.org\n",
      "c⃝Cambridge University Press 2008\n",
      "This book is in copyright. Subj\n"
     ]
    }
   ],
   "source": [
    "page = pages[3]\n",
    "print(page.page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99c184a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.10',\n",
       " 'creator': 'LaTeX with hyperref package',\n",
       " 'creationdate': '2010-10-01T15:47:05-07:00',\n",
       " 'author': 'AlexJ.SmolaandVishyS.V.N.Vishwanathan',\n",
       " 'title': 'AnIntroductiontoMachineLearning',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2010-10-01T15:47:05-07:00',\n",
       " 'trapped': '/False',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-1.40.10-2.2 (TeX Live/MacPorts 2009_6) kpathsea version 5.0.0',\n",
       " 'source': 'C:/Users/KUSHAL/OneDrive/Desktop/Notebook LM/thebook_machine_learning.pdf',\n",
       " 'total_pages': 234,\n",
       " 'page': 3,\n",
       " 'page_label': 'iv'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b9a5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ba00554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f83c6b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95fc0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/NotionDB\")\n",
    "notion_db = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "893be5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(notion_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e64511b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notion_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5cca17fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8693f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a086928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50a5863d",
   "metadata": {},
   "source": [
    "## Splitter Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4b89078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "chunk_size = 26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f2549a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "text_1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "text_2 = 'abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fee11ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa2d79af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefghijklmnopqrstuv', 'stuvwxyz']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de36046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c, d e f g h, i ,j k l', 'k l m ,n o p q r s t u v', 'u v w ,x y z']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3 = \"a b c, d e f g h, i ,j k l m ,n o p q r s t u v w ,x y z\"\n",
    "r_splitter.split_text(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9af6df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c, d e f g h, i', 'i ,j k l m', 'n o p q r s t u v w ,x y z']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator=\",\")\n",
    "c_splitter.split_text(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dfdf703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\"\n",
    "\n",
    "len(some_text)\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 450,\n",
    "    chunk_overlap = 0,\n",
    "    separator = ' '\n",
    ")\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap = 0,\n",
    "    separators= ['\\n\\n', '\\n',\"(?<=\\. )\", ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19963464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dddac676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d75b5",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f314780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "348717aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "persit_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2580a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"C:/Users/KUSHAL/OneDrive/Desktop/Notebook LM/BLOGS\", glob=\"**/*.pdf\", recursive=True)\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "26ee6e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content from 11 unique PDF files\n",
      "These files: {'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Edge_Revolution_Key_Trends_in_Edge_Computing_for_2025_and_Beyond.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Navigating_the_Evolving_Landscape_Top_Cybersecurity_Trends_for_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Quantum_Leap_Emerging_Trends_in_Quantum_Computing_for_Scientific_and_Industrial_Applications.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Green_Horizons_The_Rise_of_Sustainable_Technology_in_2025_and_Beyond.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Future_is_Now_Dominant_Cloud_Computing_Trends_of_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Ubiquitous_Ascent_of_Artificial_Intelligence_in_Daily_Life.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Beyond_the_Hype_Key_Blockchain_Technology_Trends_for_2025-2030.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Immersive_Futures_Key_Trends_in_Augmented_and_Virtual_Reality_for_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Opening_the_Black_Box_The_Critical_Rise_of_Explainable_AI_(XAI).pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\blog_1.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_AI_Revolution_and_the_Future_of_Work_Displacement,_Creation,_and_Reskilling.pdf'}\n"
     ]
    }
   ],
   "source": [
    "unique_paths = {doc.metadata[\"source\"] for doc in docs}\n",
    "num_files = len(unique_paths)\n",
    "print(f\"Loaded content from {num_files} unique PDF files\")\n",
    "print(\"These files:\", unique_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19df39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.6.15)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/216.1 MB 3.7 MB/s eta 0:00:58\n",
      "   ---------------------------------------- 2.4/216.1 MB 5.6 MB/s eta 0:00:39\n",
      "    --------------------------------------- 4.2/216.1 MB 6.6 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 6.3/216.1 MB 7.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 8.4/216.1 MB 7.9 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 10.7/216.1 MB 8.5 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 13.1/216.1 MB 8.8 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 15.2/216.1 MB 8.9 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 16.8/216.1 MB 8.7 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 17.6/216.1 MB 8.3 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 18.1/216.1 MB 7.8 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 19.1/216.1 MB 7.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 20.2/216.1 MB 7.2 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 21.2/216.1 MB 7.1 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 22.5/216.1 MB 7.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 24.1/216.1 MB 7.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 25.7/216.1 MB 7.0 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 27.0/216.1 MB 7.0 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 28.3/216.1 MB 7.0 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 29.6/216.1 MB 7.0 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 31.2/216.1 MB 7.0 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 32.8/216.1 MB 7.0 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 34.9/216.1 MB 7.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 36.7/216.1 MB 7.1 MB/s eta 0:00:26\n",
      "   ------- -------------------------------- 38.8/216.1 MB 7.3 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 40.6/216.1 MB 7.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 42.2/216.1 MB 7.3 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 43.8/216.1 MB 7.3 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 46.1/216.1 MB 7.5 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 48.8/216.1 MB 7.6 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 51.4/216.1 MB 7.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 54.0/216.1 MB 7.9 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 56.4/216.1 MB 8.0 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 59.0/216.1 MB 8.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 61.9/216.1 MB 8.2 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 64.7/216.1 MB 8.4 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 67.6/216.1 MB 8.5 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 70.3/216.1 MB 8.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 72.9/216.1 MB 8.7 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 75.8/216.1 MB 8.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 79.2/216.1 MB 9.0 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 81.5/216.1 MB 9.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 82.3/216.1 MB 9.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 84.1/216.1 MB 8.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 87.3/216.1 MB 9.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 88.6/216.1 MB 9.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 92.0/216.1 MB 9.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 95.2/216.1 MB 9.2 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 98.3/216.1 MB 9.4 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 101.4/216.1 MB 9.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 104.6/216.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 107.7/216.1 MB 9.7 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 111.7/216.1 MB 9.8 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 115.3/216.1 MB 10.0 MB/s eta 0:00:11\n",
      "   --------------------- ----------------- 119.5/216.1 MB 10.1 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 123.5/216.1 MB 10.3 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 126.6/216.1 MB 10.4 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 130.0/216.1 MB 10.4 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 134.0/216.1 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 137.4/216.1 MB 10.7 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 140.2/216.1 MB 10.7 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 143.7/216.1 MB 10.8 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 146.0/216.1 MB 10.8 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 149.2/216.1 MB 10.9 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 152.8/216.1 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 154.4/216.1 MB 10.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 158.1/216.1 MB 11.0 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 162.3/216.1 MB 11.1 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 166.2/216.1 MB 11.2 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 169.6/216.1 MB 11.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 172.8/216.1 MB 11.3 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 175.9/216.1 MB 11.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 179.0/216.1 MB 11.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 182.5/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 186.1/216.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 190.1/216.1 MB 11.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 193.7/216.1 MB 11.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 197.1/216.1 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 200.8/216.1 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 203.7/216.1 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 207.4/216.1 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  211.3/216.1 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  215.2/216.1 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 3.9/6.3 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 14.2 MB/s eta 0:00:00\n",
      "Using cached pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.4/10.7 MB 7.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.2/10.7 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.1/38.6 MB 15.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.6/38.6 MB 18.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.3/38.6 MB 19.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 15.7/38.6 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.4/38.6 MB 18.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.4/38.6 MB 18.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.0/38.6 MB 19.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.6 MB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, Pillow, networkx, joblib, torch, scikit-learn, sentence-transformers\n",
      "\n",
      "   ----------------------------------------  0/10 [mpmath]\n",
      "   ----------------------------------------  0/10 [mpmath]\n",
      "   ----------------------------------------  0/10 [mpmath]\n",
      "   ----------------------------------------  0/10 [mpmath]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   -------- -------------------------------  2/10 [sympy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ------------ ---------------------------  3/10 [scipy]\n",
      "   ---------------- -----------------------  4/10 [Pillow]\n",
      "   ---------------- -----------------------  4/10 [Pillow]\n",
      "   ---------------- -----------------------  4/10 [Pillow]\n",
      "   ---------------- -----------------------  4/10 [Pillow]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   -------------------- -------------------  5/10 [networkx]\n",
      "   ------------------------ ---------------  6/10 [joblib]\n",
      "   ------------------------ ---------------  6/10 [joblib]\n",
      "   ------------------------ ---------------  6/10 [joblib]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   -------------------------------- -------  8/10 [scikit-learn]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ---------------------------------------- 10/10 [sentence-transformers]\n",
      "\n",
      "Successfully installed Pillow-11.3.0 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 scikit-learn-1.7.0 scipy-1.16.0 sentence-transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03f2bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.15-cp39-abi3-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.1-cp311-cp311-win_amd64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (2.3.1)\n",
      "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (0.21.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.33.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kushal\\anaconda3\\envs\\fapienv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Downloading chromadb-1.0.15-cp39-abi3-win_amd64.whl (19.5 MB)\n",
      "   ---------------------------------------- 0.0/19.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.4/19.5 MB 33.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 7.6/19.5 MB 19.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 11.8/19.5 MB 19.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.1/19.5 MB 22.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.5/19.5 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading mmh3-5.1.0-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.7/12.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.7 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.7 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.9/12.7 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/12.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.7 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.7 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.5/12.7 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pybase64-1.4.1-cp311-cp311-win_amd64.whl (36 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-win_amd64.whl (88 kB)\n",
      "Downloading watchfiles-1.1.0-cp311-cp311-win_amd64.whl (292 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53916 sha256=dcc9afac67ce716b97789929cb8e87e103f0df0779459254ade9d6cfe2ccdcb5\n",
      "  Stored in directory: c:\\users\\kushal\\appdata\\local\\pip\\cache\\wheels\\a3\\01\\bd\\4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, zipp, pyreadline3, pyproject_hooks, pybase64, pyasn1, opentelemetry-proto, oauthlib, mmh3, importlib-resources, httptools, grpcio, googleapis-common-protos, cachetools, bcrypt, backoff, watchfiles, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, importlib-metadata, humanfriendly, build, opentelemetry-api, google-auth, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "\n",
      "   ----------------------------------------  0/36 [pypika]\n",
      "   -- -------------------------------------  2/36 [durationpy]\n",
      "   ---- -----------------------------------  4/36 [pyreadline3]\n",
      "   ---- -----------------------------------  4/36 [pyreadline3]\n",
      "   ----- ----------------------------------  5/36 [pyproject_hooks]\n",
      "   ------- --------------------------------  7/36 [pyasn1]\n",
      "   ------- --------------------------------  7/36 [pyasn1]\n",
      "   -------- -------------------------------  8/36 [opentelemetry-proto]\n",
      "   -------- -------------------------------  8/36 [opentelemetry-proto]\n",
      "   ---------- -----------------------------  9/36 [oauthlib]\n",
      "   ---------- -----------------------------  9/36 [oauthlib]\n",
      "   ---------- -----------------------------  9/36 [oauthlib]\n",
      "   ----------- ---------------------------- 10/36 [mmh3]\n",
      "   ------------ --------------------------- 11/36 [importlib-resources]\n",
      "   -------------- ------------------------- 13/36 [grpcio]\n",
      "   -------------- ------------------------- 13/36 [grpcio]\n",
      "   -------------- ------------------------- 13/36 [grpcio]\n",
      "   -------------- ------------------------- 13/36 [grpcio]\n",
      "   --------------- ------------------------ 14/36 [googleapis-common-protos]\n",
      "   --------------- ------------------------ 14/36 [googleapis-common-protos]\n",
      "   --------------- ------------------------ 14/36 [googleapis-common-protos]\n",
      "   --------------- ------------------------ 14/36 [googleapis-common-protos]\n",
      "   --------------- ------------------------ 14/36 [googleapis-common-protos]\n",
      "   ------------------ --------------------- 17/36 [backoff]\n",
      "   --------------------- ------------------ 19/36 [rsa]\n",
      "   ---------------------- ----------------- 20/36 [requests-oauthlib]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 21/36 [pyasn1-modules]\n",
      "   ------------------------ --------------- 22/36 [posthog]\n",
      "   ------------------------ --------------- 22/36 [posthog]\n",
      "   ------------------------ --------------- 22/36 [posthog]\n",
      "   -------------------------- ------------- 24/36 [importlib-metadata]\n",
      "   --------------------------- ------------ 25/36 [humanfriendly]\n",
      "   ---------------------------- ----------- 26/36 [build]\n",
      "   ------------------------------ --------- 27/36 [opentelemetry-api]\n",
      "   ------------------------------ --------- 27/36 [opentelemetry-api]\n",
      "   ------------------------------- -------- 28/36 [google-auth]\n",
      "   ------------------------------- -------- 28/36 [google-auth]\n",
      "   ------------------------------- -------- 28/36 [google-auth]\n",
      "   -------------------------------- ------- 29/36 [coloredlogs]\n",
      "   --------------------------- ----- 30/36 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 30/36 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 30/36 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 30/36 [opentelemetry-semantic-conventions]\n",
      "   --------------------------- ----- 30/36 [opentelemetry-semantic-conventions]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ---------------------------------- ----- 31/36 [onnxruntime]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ----------------------------------- ---- 32/36 [kubernetes]\n",
      "   ------------------------------------ --- 33/36 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 33/36 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 33/36 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 33/36 [opentelemetry-sdk]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   -------------------------------------- - 35/36 [chromadb]\n",
      "   ---------------------------------------- 36/36 [chromadb]\n",
      "\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 flatbuffers-25.2.10 google-auth-2.40.3 googleapis-common-protos-1.70.0 grpcio-1.73.1 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 kubernetes-33.1.0 mmh3-5.1.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 posthog-5.4.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybase64-1.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rsa-4.9.1 watchfiles-1.1.0 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content from 11 unique PDF files\n",
      "These files: {'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Edge_Revolution_Key_Trends_in_Edge_Computing_for_2025_and_Beyond.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Navigating_the_Evolving_Landscape_Top_Cybersecurity_Trends_for_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Quantum_Leap_Emerging_Trends_in_Quantum_Computing_for_Scientific_and_Industrial_Applications.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Green_Horizons_The_Rise_of_Sustainable_Technology_in_2025_and_Beyond.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Future_is_Now_Dominant_Cloud_Computing_Trends_of_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_Ubiquitous_Ascent_of_Artificial_Intelligence_in_Daily_Life.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Beyond_the_Hype_Key_Blockchain_Technology_Trends_for_2025-2030.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Immersive_Futures_Key_Trends_in_Augmented_and_Virtual_Reality_for_2025.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\Opening_the_Black_Box_The_Critical_Rise_of_Explainable_AI_(XAI).pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\blog_1.pdf', 'C:\\\\Users\\\\KUSHAL\\\\OneDrive\\\\Desktop\\\\Notebook LM\\\\BLOGS\\\\The_AI_Revolution_and_the_Future_of_Work_Displacement,_Creation,_and_Reskilling.pdf'}\n",
      "Total chunks: 193\n",
      "Embedding and storage complete.\n",
      "\n",
      "Top Unique Results (with diversity):\n",
      "\n",
      "Result 1:\n",
      "Source: C:\\Users\\KUSHAL\\OneDrive\\Desktop\\Notebook LM\\BLOGS\\The_Future_is_Now_Dominant_Cloud_Computing_Trends_of_2025.pdf\n",
      "Content: on-board processing for deep learning, is a testament to their commitment to making AI \n",
      "more accessible for developers and researchers.\n",
      "Google, with its deep roots in AI research, is also heavily invested in this area. Google Cloud \n",
      "oﬀers a comprehensive suite of AI and ML products, including tools for building \n",
      "conversational AI, analyzing unstructured data, and creating custom machine learning \n",
      "models. The recent rollout of Google Lens, which allows users to search what they see using \n",
      "their c\n",
      "\n",
      "Result 2:\n",
      "Source: C:\\Users\\KUSHAL\\OneDrive\\Desktop\\Notebook LM\\BLOGS\\The_Ubiquitous_Ascent_of_Artificial_Intelligence_in_Daily_Life.pdf\n",
      "Content: 'Alexa' or 'Hey Google'), clear microphone arrays for accurate voice command \n",
      "interpretation, and friendly, naturalistic responses from the AI assistants. Even when the AI \n",
      "makes mistakes, these interactions provide valuable data for the system to learn and \n",
      "improve its language models over time, creating a virtuous cycle of continuous \n",
      "enhancement.\n",
      "Similarly, computer vision AI, which enables features like automatic photo tagging and \n",
      "visual search, relies on extensive labeled datasets and soph\n",
      "\n",
      "Result 3:\n",
      "Source: C:\\Users\\KUSHAL\\OneDrive\\Desktop\\Notebook LM\\BLOGS\\The_Future_is_Now_Dominant_Cloud_Computing_Trends_of_2025.pdf\n",
      "Content: is another dominant trend that is reshaping the technological landscape. Cloud providers \n",
      "are increasingly embedding AI and ML capabilities directly into their services, making these \n",
      "powerful technologies more accessible and aﬀordable for businesses of all sizes. This \n",
      "integration enables organizations to leverage advanced analytics, predictive modeling, and \n",
      "automation without the need for extensive in-house expertise or signiﬁcant upfront \n",
      "investment in infrastructure.\n",
      "AWS, for example, has b\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Required installation instructions:\n",
    "# pip install sentence-transformers chromadb\n",
    "\n",
    "# Load PDFs\n",
    "directory_path = \"C:/Users/KUSHAL/OneDrive/Desktop/Notebook LM/BLOGS\"\n",
    "loader = PyPDFDirectoryLoader(directory_path, glob=\"**/*.pdf\", recursive=True)\n",
    "docs = loader.load()\n",
    "\n",
    "# Confirm loaded files\n",
    "unique_paths = {doc.metadata[\"source\"] for doc in docs}\n",
    "print(f\"Loaded content from {len(unique_paths)} unique PDF files\")\n",
    "print(\"These files:\", unique_paths)\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# Create sentence-transformers embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store embeddings in Chroma\n",
    "persist_directory = \"chroma_store\"\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=persist_directory)\n",
    "vectorstore.persist()\n",
    "print(\"Embedding and storage complete.\")\n",
    "\n",
    "# Querying top-k diverse results using MMR\n",
    "def query_docs(query, k=3, fetch_k=15):\n",
    "    raw_results = vectorstore.max_marginal_relevance_search(query, k=fetch_k)\n",
    "    seen_sources = set()\n",
    "    unique_results = []\n",
    "\n",
    "    for doc in raw_results:\n",
    "        key = (doc.metadata.get(\"source\", \"\"), hash(doc.page_content))\n",
    "        if key not in seen_sources:\n",
    "            unique_results.append(doc)\n",
    "            seen_sources.add(key)\n",
    "        if len(unique_results) == k:\n",
    "            break\n",
    "\n",
    "    print(\"\\nTop Unique Results (with diversity):\")\n",
    "    for i, doc in enumerate(unique_results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(\"Source:\", doc.metadata.get(\"source\", \"Unknown\"))\n",
    "        print(\"Content:\", doc.page_content[:500])\n",
    "\n",
    "# Try again\n",
    "query_docs(\"Explain the role of neural networks in deep learning, and how they differ from traditional machine learning algorithms or data science approaches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab973fb",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e00d387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk Size: 256 tokens\n",
      "Q1: What is the main goal of the book?\n",
      "A: <think>\n",
      "Okay, the user is asking about the main goal of the book based on the provided context. Let me look through the given information again.\n",
      "\n",
      "The context includes several customer reviews and the preface of the book. The reviews mention that the book is used as a textbook in machine learning and data mining classes. It's noted as a helpful resource for students, an excellent addition to a Machine Learning class, and a good reference. The preface states that the book is a textbook and mentions a bias towards easily accessible works for simplification.\n",
      "\n",
      "From this, the main goal seems to be serving as a textbook for students in these fields. The authors aimed to make the material accessible, possibly prioritizing clarity and ease of understanding over citing original sources. They want students to focus on learning the concepts rather than getting bogged down by the original references. The reviews also highlight its usefulness in classes and as a study aid, reinforcing that the goal is educational support for students in data mining and machine learning.\n",
      "</think>\n",
      "\n",
      "The main goal of the book, as indicated in the preface and customer reviews, is to serve as an **accessible and practical textbook for students studying data mining and machine learning**. The authors aim to simplify complex concepts by prioritizing easily understandable references over original sources, ensuring the material is approachable for learners. Reviews also highlight its utility as a supplementary resource in classes, emphasizing clarity and its role in supporting students' understanding of key techniques in the field.\n",
      "\n",
      "Q2: Explain one key concept in machine learning discussed in the PDF.\n",
      "A: <think>\n",
      "Okay, let's see. The user asked me to explain one key concept in machine learning discussed in the provided PDF. First, I need to look at the context given. The PDF seems to be an introduction to machine learning, and the content is from the beginning of a book or chapter.\n",
      "\n",
      "Looking at the text, the introduction mentions that machine learning is a mainstay of information technology and is central in our lives. It talks about the increasing availability of data and the importance of smart data analysis. The purpose of the chapter is to provide an overview of applications with machine learning problems, bring order to the problems, discuss basic tools from statistics and probability, and outline basic algorithms for classification. It also mentions that more sophisticated tools and general problems are discussed later.\n",
      "\n",
      "So the key concepts mentioned here are the overview of applications, the use of statistical and probabilistic tools, and classification as an important problem. The user wants one key concept. Classification seems to be highlighted as an important problem in the text. The introduction says that the chapter will outline basic algorithms for classification. Since the user is asking for a key concept, classification is a fundamental concept in machine learning. The text also mentions that classification is an important problem to solve, which aligns with the user's request.\n",
      "\n",
      "I need to make sure that the answer is based on the provided context. The text doesn't go into details about classification algorithms yet, just mentions that they will be outlined. So the key concept here is classification as a problem in machine learning, which involves categorizing data into predefined classes. Examples might include spam detection, image recognition, etc., but the context doesn't specify examples. However, the text does state that classification is an important problem, so the answer should focus on that aspect as the key concept discussed in the introduction.\n",
      "</think>\n",
      "\n",
      "The PDF highlights **classification** as a key concept in machine learning. It describes classification as an important problem where the goal is to assign data points to predefined categories or classes. The text emphasizes that basic algorithms for solving classification tasks will be outlined in the chapter, positioning it as a foundational problem in the field. Classification is central to applications like spam detection, image recognition, and medical diagnosis, where data must be organized into distinct groups based on patterns learned from training examples.\n",
      "\n",
      "Q3: What are common challenges in machine learning mentioned in the book?\n",
      "A: <think>\n",
      "Okay, let's see. The user is asking about the common challenges in machine learning mentioned in the book. I need to check the provided context to find any references to challenges.\n",
      "\n",
      "Looking at the context, there are three parts. The first two parts are the introduction from a machine learning book. They talk about the overview of applications, statistics and probability tools, and classification algorithms. But they don't mention specific challenges. The third part includes some user reviews praising the book but not discussing challenges. \n",
      "\n",
      "The user reviews mention that the book is a good introduction, helpful, and well-written, but none of them point out challenges. The main content from the book's introduction doesn't list challenges either. It focuses on applications, problem-solving approaches, and basic algorithms. There's no mention of issues like overfitting, data scarcity, computational complexity, or other common machine learning challenges in the given text.\n",
      "\n",
      "Since the context doesn't provide information about challenges in machine learning from the book, I should inform the user that the answer isn't available here. The user might need to refer to other sections of the book or external sources for that information.\n",
      "</think>\n",
      "\n",
      "The provided context does not explicitly mention specific challenges in machine learning discussed in the book. The excerpts focus on introducing the role of machine learning in technology, its applications, and the structure of the book (e.g., covering classification algorithms, statistics, and probability tools). For detailed challenges (e.g., overfitting, data scarcity, computational complexity, or generalization issues), you would need to consult other sections of the book or additional resources on machine learning.\n",
      "\n",
      "\n",
      "Chunk Size: 512 tokens\n",
      "Q1: What is the main goal of the book?\n",
      "A: <think>\n",
      "Okay, let's see. The user is asking about the main goal of the book based on the provided context. The context includes several customer reviews and some text from the book's preface.\n",
      "\n",
      "First, looking at the customer reviews: One mentions it's a great introduction for data mining and machine learning classes. Another says it's excellently written and a welcome addition to a Machine Learning class. Another reviewer notes it's a brief introduction to ML and mentions the statistical part being a subset of mathematical statistics. There's also a comment about it being an excellent reference book but needing more figures.\n",
      "\n",
      "From the preface, the authors mention that they biased their references towards accessible works rather than original sources, aiming to simplify access to topics. They encourage readers to check cited works for original inventors if interested.\n",
      "\n",
      "Putting this together, the main goal seems to be providing an accessible introduction to machine learning and data mining. The reviews highlight it as a textbook used in classes, praised for its clarity and helpfulness. The preface emphasizes accessibility over original references, which supports the idea that the book is intended as an introductory resource. There's no explicit mention of advanced topics or specialized areas, so the focus is likely on foundational knowledge. The user might also be looking for confirmation that the book serves as both a textbook and a reference, given the reviews. However, the key point is that the main goal is to introduce students to the field with clear explanations, even if it's not the most original in references. The answer should reflect that the book aims to be an accessible and helpful resource for students in ML and data mining courses.\n",
      "</think>\n",
      "\n",
      "The main goal of the book, as indicated by the provided context, is to serve as an **accessible introduction to machine learning (ML) and data mining** for students. The reviews highlight its use as a textbook in classes, emphasizing clarity, helpful explanations, and practicality for learners. Key points supporting this include:\n",
      "\n",
      "1. **Educational Focus**: Multiple reviewers mention it is used in ML and data mining courses, describing it as a \"great introduction\" and \"excellently written\" for students.\n",
      "2. **Simplified Access**: The preface states the authors prioritized accessible references over original sources to make topics easier to understand, aligning with an introductory audience.\n",
      "3. **Practical Utility**: Reviewers praise its role as a supplementary or required textbook, noting its value for studying foundational ML concepts, though some suggest it is brief or lacks depth in certain areas (e.g., figures).\n",
      "\n",
      "While not explicitly stated, the book appears aimed at **undergraduate or graduate students** seeking foundational knowledge in ML and data mining, balancing theoretical concepts with practical applications.\n",
      "\n",
      "Q2: Explain one key concept in machine learning discussed in the PDF.\n",
      "A: <think>\n",
      "Okay, let's see. The user wants me to explain one key concept in machine learning based on the provided PDF content. First, I need to recall what the PDF said. The introduction mentions that machine learning has become central to IT and our lives, and it talks about the purpose of the chapter being an overview of applications and organizing the problems.\n",
      "\n",
      "The key concepts mentioned include applications like web page ranking, using statistics and probability as foundational tools, and classification as an important problem. The PDF also emphasizes reducing disparate problems to prototypes and solving them with algorithms.\n",
      "\n",
      "Hmm, the user asked for one key concept. The introduction discusses classification as a basic problem, so that's a possible candidate. Also, the approach of framing problems into prototypes to apply machine learning solutions is another key concept. Maybe the main idea is about structuring problems into standard categories so that algorithms can be applied effectively. \n",
      "\n",
      "The text says, \"much of the art of machine learning is to reduce a range of fairly disparate problems to a set of fairly narrow prototypes.\" So this prototyping or problem reduction is a key concept. It's about identifying common underlying problems across different applications. For example, classification is a prototype where you categorize inputs into classes, like spam detection or image recognition. This abstraction allows using standard algorithms once the problem is framed correctly.\n",
      "\n",
      "Alternatively, the introduction mentions basic tools from statistics and probability. But the user might be looking for a concept rather than tools. The classification problem is explicitly highlighted as an important problem in the introduction. The answer should pick one. Since the text says \"classification\" is an important problem, and they outline basic algorithms for it, that's a key concept. \n",
      "\n",
      "So, I'll go with classification as the key concept. The explanation should mention that it's about categorizing data into predefined classes, examples like spam filtering, and how ML algorithms learn from data to make these predictions. Also, maybe mention that it's a fundamental problem in ML and forms the basis for more complex tasks. Need to keep it concise and based on the provided text.\n",
      "</think>\n",
      "\n",
      "One key concept in machine learning discussed in the text is **classification**. The text highlights classification as a fundamental problem in machine learning, where the goal is to assign input data (e.g., emails, images) to predefined categories or classes. For example, a classification algorithm might determine whether an email is \"spam\" or \"not spam.\" The introduction emphasizes that classification is a critical task, and basic algorithms for solving it are introduced in the chapter. The text also underscores the importance of framing diverse problems into standardized prototypes like classification, enabling the application of reusable solutions and statistical tools. This abstraction allows machine learning systems to generalize across different applications while relying on core principles like probability and data analysis.\n",
      "\n",
      "Q3: What are common challenges in machine learning mentioned in the book?\n",
      "A: <think>\n",
      "Okay, the user is asking about the common challenges in machine learning mentioned in the book. Let me look through the provided context to find any references to challenges.\n",
      "\n",
      "First, the introduction mentions that machine learning is a central part of IT and discusses applications like web page ranking. It talks about reducing problems to prototypes and solving them with algorithms. However, the specific challenges aren't detailed in the excerpts provided. The context includes information about the structure of the book, such as discussing basic tools from statistics and probability, classification algorithms, and more sophisticated tools later. \n",
      "\n",
      "The editorial reviews mention that the book covers key algorithms, theoretical issues like how learning performance varies with training examples and appropriate algorithms for tasks. But the user's question is about challenges, which might include things like data quality, overfitting, computational complexity, etc. However, the provided text doesn't explicitly list these challenges. Since the answer needs to be based solely on the given context, and there's no direct mention of challenges beyond the general discussion of problem-solving and algorithm selection, I should conclude that the information isn't available here. Therefore, I need to inform the user that the specific challenges aren't mentioned in the provided excerpts.\n",
      "</think>\n",
      "\n",
      "The provided excerpts from the text do not explicitly mention specific challenges in machine learning. The content focuses on introducing machine learning applications (e.g., web page ranking), the role of statistics and probability in framing problems, and the structure of the book (e.g., classification algorithms, problem formalization). For detailed discussions of challenges (e.g., overfitting, data scarcity, computational complexity), you would need to refer to later chapters or sections of the book that are not included in the provided context.\n",
      "\n",
      "\n",
      "Chunk Size: 1024 tokens\n",
      "Q1: What is the main goal of the book?\n",
      "A: <think>\n",
      "Okay, let's tackle this query. The user is asking about the main goal of the book based on the provided context. Let me look through the given information.\n",
      "\n",
      "First, I see multiple customer reviews mentioning that the book is used as a textbook in classes, both for data mining and machine learning. One review says it's a \"great introduction book\" and another calls it a \"welcome addition\" to a Machine Learning class. Another review points out that the statistical part of ML is a subset of mathematical statistics, which might indicate the book covers foundational topics.\n",
      "\n",
      "The preface mentions that the book is biased towards easily accessible work rather than original references, aiming to simplify access to topics. This suggests the goal is to make the material more approachable for students, possibly as a textbook. The fact that it's used in classes and described as helpful for study supports that the main goal is educational, providing an introductory resource for students in these fields.\n",
      "\n",
      "I don't see any direct mention of advanced topics or specialized areas, so the focus seems to be on foundational knowledge. The user might be a student looking for an overview or someone considering the book for a course. They might need to know if it's suitable as a primary text or supplementary material. The answer should highlight the educational purpose, accessibility, and its use in academic settings.\n",
      "</think>\n",
      "\n",
      "The main goal of the book, as indicated by the provided context and reviews, is to serve as an **accessible and practical textbook for students studying data mining and machine learning**. It aims to:  \n",
      "\n",
      "1. **Provide foundational knowledge**: Reviews describe it as a \"great introduction\" to machine learning (ML) and data mining, emphasizing its utility for students in these fields.  \n",
      "2. **Support academic learning**: It has been used as a textbook in classes, with students noting its helpfulness even when not required, and professors acknowledging its value as a supplementary resource.  \n",
      "3. **Simplify complex concepts**: The preface mentions a bias toward accessible explanations over original technical references, prioritizing clarity for learners.  \n",
      "\n",
      "While the book is praised for its readability and structure, some reviewers note it is more of an introductory or reference text rather than an in-depth exploration of advanced topics.\n",
      "\n",
      "Q2: Explain one key concept in machine learning discussed in the PDF.\n",
      "A: <think>\n",
      "Okay, so the user is asking me to explain one key concept in machine learning based on the provided PDF text. Let me look through the text again to make sure I pick the right concept.\n",
      "\n",
      "The introduction mentions that machine learning is central in IT and part of our daily lives. It talks about the importance of smart data analysis growing with more data availability. The chapter aims to give an overview of applications, structure problems, discuss statistical tools, and outline classification algorithms.\n",
      "\n",
      "In section 1.1, it says machine learning has many guises but the key is formalizing problems to avoid reinventing the wheel. The art is reducing disparate problems to prototypes, and the science is solving them with good guarantees. Applications mentioned include web page ranking, where the search engine needs to sort web pages by relevance.\n",
      "\n",
      "Hmm, the key concept here might be about problem formalization or reduction. The text emphasizes that a big part of machine learning is structuring problems into standard prototypes. That way, you don't have to start from scratch each time. So, the idea is to abstract different problems into a few standard forms. For example, classification, regression, clustering, etc. The PDF specifically mentions classification in the context of basic algorithms.\n",
      "\n",
      "Wait, the user might be looking for something like supervised learning, unsupervised learning, or maybe the concept of generalization. But the text here doesn't go into those details yet. The main point made is about problem formalization. The text says, \"reduce a range of fairly disparate problems to a set of fairly narrow prototypes.\" So the key concept here is problem abstraction or formalizing problems into standard types.\n",
      "\n",
      "Another angle is the application of statistical tools and probability theory as the language for machine learning. But the example given is about classification algorithms. However, the introduction's main thrust is about structuring problems. So maybe the key concept is the abstraction of problems into standard forms, allowing the application of common algorithms. That way, different applications (like web page ranking) can be approached using the same underlying methods once they're formalized correctly.\n",
      "\n",
      "I need to make sure that this is the main idea. The text also mentions that after formalizing, you can use basic tools like statistics and probability, then move to algorithms like classification. But the key concept in the introduction is the process of reducing various problems to standard prototypes. That's the art of machine learning as per the text. So the answer should focus on that concept: structuring diverse problems into standard problem types to apply existing solutions.\n",
      "</think>\n",
      "\n",
      "A key concept discussed in the PDF is **problem formalization in machine learning**. The text emphasizes that the \"art\" of machine learning lies in reducing diverse, disparate problems into a set of standardized prototypes (e.g., classification, regression, clustering). This abstraction allows developers to avoid reinventing solutions for every application by framing problems in a way that aligns with established methodologies. For example, tasks like web page ranking are formalized as a problem of sorting webpages by relevance, which can then be addressed using statistical and algorithmic tools. This structured approach enables reusable solutions and ensures that solutions are grounded in mathematical rigor (e.g., probability theory and statistics). The text highlights that this formalization is critical to making machine learning problems tractable and scalable.\n",
      "\n",
      "Q3: What are common challenges in machine learning mentioned in the book?\n",
      "A: <think>\n",
      "Okay, the user is asking about common challenges in machine learning mentioned in the book. Let me check the provided context to see if there's any information on that.\n",
      "\n",
      "Looking through the context, there's a lot of repetition of the introduction and sections about the overview of applications and basic tools. But when I scan through the content, there's no explicit mention of challenges. The sections talk about applications like web page ranking, the need for smart data analysis, and the structure of the book, but not specific challenges. \n",
      "\n",
      "The user might be expecting challenges like overfitting, data scarcity, computational complexity, or others typical in ML. However, since the provided context doesn't discuss these issues, I can't confirm they're mentioned in the book based on the given text. The customer reviews also don't touch on challenges, just the book's coverage and audience.\n",
      "\n",
      "Since there's no information in the context about challenges, I should inform the user that the provided text doesn't mention specific challenges. Maybe suggest looking into sections on problem formalization or later chapters for more details, but based on the given context, I can't provide the answer. It's important to be honest and not make up information.\n",
      "</think>\n",
      "\n",
      "The provided context does not explicitly mention specific challenges in machine learning discussed in the book. The text focuses on introducing machine learning applications, problem formalization, and foundational tools like statistics and algorithms for classification. For detailed discussions on challenges (e.g., overfitting, data scarcity, computational complexity), you would need to refer to later sections of the book or additional resources.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY_temp_1\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"C:/Users/KUSHAL/OneDrive/Desktop/Notebook LM/thebook_machine_learning.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Create embeddings instance\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to build retriever using Chroma\n",
    "def build_retriever(chunks, collection_name):\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=f\"./chroma_store_{collection_name}\"\n",
    "    )\n",
    "    retriever = db.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "# Function to split and return chunks for each chunk size\n",
    "def get_chunks(chunk_size):\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=0\n",
    "    )\n",
    "    return splitter.split_documents(pages)\n",
    "\n",
    "# Build chunked documents and retrievers\n",
    "chunks_256 = get_chunks(256)\n",
    "chunks_512 = get_chunks(512)\n",
    "chunks_1024 = get_chunks(1024)\n",
    "retriever_256 = build_retriever(chunks_256, \"ml_chunks_256\")\n",
    "retriever_512 = build_retriever(chunks_512, \"ml_chunks_512\")\n",
    "retriever_1024 = build_retriever(chunks_1024, \"ml_chunks_1024\")\n",
    "\n",
    "# Initialize LLM for QA\n",
    "llm = ChatGroq(model_name=\"qwen/qwen3-32b\", api_key=GROQ_API_KEY)\n",
    "\n",
    "# Create RetrievalQA chains\n",
    "qa_256 = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_256)\n",
    "qa_512 = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_512)\n",
    "qa_1024 = RetrievalQA.from_chain_type(llm=llm, retriever=retriever_1024)\n",
    "\n",
    "# Define evaluation questions\n",
    "questions = [\n",
    "    \"What is the main goal of the book?\",\n",
    "    \"Explain one key concept in machine learning discussed in the PDF.\",\n",
    "    \"What are common challenges in machine learning mentioned in the book?\"\n",
    "]\n",
    "\n",
    "# Collect answers for each chunk size\n",
    "results = {\"256\": [], \"512\": [], \"1024\": []}\n",
    "for q in questions:\n",
    "    results[\"256\"].append(qa_256.run(q))\n",
    "    results[\"512\"].append(qa_512.run(q))\n",
    "    results[\"1024\"].append(qa_1024.run(q))\n",
    "\n",
    "# Output results\n",
    "for size in [\"256\", \"512\", \"1024\"]:\n",
    "    print(f\"\\nChunk Size: {size} tokens\")\n",
    "    for i, answer in enumerate(results[size]):\n",
    "        print(f\"Q{i+1}: {questions[i]}\\nA: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e811cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fapienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
